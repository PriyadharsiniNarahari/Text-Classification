{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Reading data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>ans887.txt</td>\n",
       "      <td>Most cases of fracture of clavicle are managed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>ans1355.txt</td>\n",
       "      <td>The heavy bleeding that you are experiencing c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>a54273.txt</td>\n",
       "      <td>Eat wheat bread instead of white. Eat carrots ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>a61341.txt</td>\n",
       "      <td>i find surfing for naughty pictures and videos...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>a31636.txt</td>\n",
       "      <td>I ALWAYS GET RID OF MINE BY HOLDING MY BREATH ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>a24790.txt</td>\n",
       "      <td>Cuz they are hungry. Only females bite people,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>a69413.txt</td>\n",
       "      <td>Good luck m8 and get well soon but watch out f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>ans972.txt</td>\n",
       "      <td>Palpitations are the sensation that your heart...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>a24722.txt</td>\n",
       "      <td>Good question! As for myself, Cant sleep due t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>a24286.txt</td>\n",
       "      <td>Just lie about it-- but wait, youre doing that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>a24826.txt</td>\n",
       "      <td>Google your ideal caloric intake based on your...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>ans366.txt</td>\n",
       "      <td>Herpes zoster, also called shingles, is a reac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>ans746.txt</td>\n",
       "      <td>It is normal to get dizzy when you suddenly ge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>a61309.txt</td>\n",
       "      <td>Muscle and tissue bruise, bones break - depend...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>ans1831.txt</td>\n",
       "      <td>Your swollen thigh could be the result of cell...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>ans1352.txt</td>\n",
       "      <td>The headaches, dizziness and tiredness may be ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>a61258.txt</td>\n",
       "      <td>It is a proven fact that alcohol is a depressa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>a69434.txt</td>\n",
       "      <td>When you are sick your priority is to get/feel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file                                               text  class\n",
       "1749   ans887.txt  Most cases of fracture of clavicle are managed...      1\n",
       "396   ans1355.txt  The heavy bleeding that you are experiencing c...      1\n",
       "867    a54273.txt  Eat wheat bread instead of white. Eat carrots ...      0\n",
       "1041   a61341.txt  i find surfing for naughty pictures and videos...      0\n",
       "712    a31636.txt  I ALWAYS GET RID OF MINE BY HOLDING MY BREATH ...      0\n",
       "558    a24790.txt  Cuz they are hungry. Only females bite people,...      0\n",
       "1281   a69413.txt  Good luck m8 and get well soon but watch out f...      0\n",
       "1844   ans972.txt  Palpitations are the sensation that your heart...      1\n",
       "490    a24722.txt  Good question! As for myself, Cant sleep due t...      0\n",
       "54     a24286.txt  Just lie about it-- but wait, youre doing that...      0\n",
       "594    a24826.txt  Google your ideal caloric intake based on your...      0\n",
       "1171   ans366.txt  Herpes zoster, also called shingles, is a reac...      1\n",
       "1593   ans746.txt  It is normal to get dizzy when you suddenly ge...      1\n",
       "1010   a61309.txt  Muscle and tissue bruise, bones break - depend...      0\n",
       "925   ans1831.txt  Your swollen thigh could be the result of cell...      1\n",
       "393   ans1352.txt  The headaches, dizziness and tiredness may be ...      1\n",
       "961    a61258.txt  It is a proven fact that alcohol is a depressa...      0\n",
       "1302   a69434.txt  When you are sick your priority is to get/feel...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "def data2df (path, label):\n",
    "    file, text = [], []\n",
    "    for f in os.listdir(path):\n",
    "        file.append(f)\n",
    "        fhr = open(path+f, 'r', encoding='utf-8', errors='ignore') \n",
    "        t = fhr.read()\n",
    "        text.append(t)\n",
    "        fhr.close()\n",
    "    return(pd.DataFrame({'file': file, 'text': text, 'class':label}))\n",
    "\n",
    "dfnonpro = data2df('HealthProNonPro/NonPro/', 0) # Pro\n",
    "dfpro = data2df('HealthProNonPro/Pro/', 1) # Non-Pro\n",
    "\n",
    "df = pd.concat([dfnonpro, dfpro], axis=0)\n",
    "df.sample(frac=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Setting up the data for Training/Testing. Using20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['text'], df['class']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "Xtrain = Xtrain.copy()\n",
    "Xtest = Xtest.copy()\n",
    "ytrain = ytrain.copy()\n",
    "ytest = ytest.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439     well how old is he? just to be safe i would ta...\n",
       "807     What you may be experiencing is post-concussio...\n",
       "394     The healing time after the trauma or muscle co...\n",
       "87                 I HAVE AN ELLIPITICAL AND I DO 30 MIN \n",
       "1153    A urinary test for pregnancy should be positiv...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Using Spacy to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# once you’ve downloaded and installed a model, you can load it via spacy.load(). \n",
    "# spacy.load() returns a Language object containing all components and data needed to process text. \\\n",
    "# the Language object is typically called nlp. \n",
    "nlp = spacy.load(\"en_core_web_md\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    old safe er soon possible 106 temp incredibaly...\n",
       "1    experience post concussion syndrome complex di...\n",
       "2    healing time trauma muscle contusion vary pati...\n",
       "3                                   ellipitical 30 min\n",
       "4    urinary test pregnancy positive pregnancy week...\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def custom_tokenizer(doc):\n",
    "\n",
    "    # clean up text\n",
    "    tokens = [token.lemma_.lower() # lemmatize and lower-case \n",
    "                        for token in doc \n",
    "                               if (\n",
    "                                    len(token) >= 2 and # only preserve tokens that are 2 or more characters long\n",
    "                                    #token.pos_ in ['PROPN', 'NOUN', 'ADJ', 'VERB', 'ADV'] and # only preserve specific pos\n",
    "                                    #token.text in nlp.vocab and # check if token in vocab\n",
    "                                    #token.is_alpha and # only preserve tokens that are fully alpha (not numeric or alpha-numeric)\n",
    "                                    #not token.is_digit and # get rid of tokens that are fully numeric\n",
    "                                    not token.is_punct and # get rid of tokens that are punctuations\n",
    "                                    not token.is_space and # get rid of tokens that are spaces\n",
    "                                    not token.is_stop # get rid of tokens that are stop words\n",
    "                                )\n",
    "                   ]\n",
    "\n",
    "    # return cleaned-up text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=['parser', 'ner'])\n",
    "nlpcorpus = nlp.pipe(corpus)\n",
    "clean_corpus = [custom_tokenizer(text) for text in nlpcorpus]\n",
    "Xtrain = pd.Series(clean_corpus)\n",
    "Xtrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Setting up a Pipeline with TfidfVectorizer and Naïve Bayes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "clf = Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
    "                      ('nb', MultinomialNB())])\n",
    "\n",
    "clf = clf.fit(Xtrain,ytrain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Grid Search with 4-fold Cross Validation to search for the best values for the following two hyper-parameters (and any additional hyper parameters you may want to tune):1)\tsublinear_tf in TfidfVectorizer\t2)alpha in Naïve Bayes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'tfidf__sublinear_tf': (True, False), \n",
    "    'nb__alpha':[0.1,0.25,0.5,1] \n",
    "}\n",
    "gscv = GridSearchCV(clf, param_grid, cv=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_acc...\n",
       "                                                        sublinear_tf=False,\n",
       "                                                        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                        tokenizer=None,\n",
       "                                                        use_idf=True,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('nb',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'nb__alpha': [0.1, 0.25, 0.5, 1],\n",
       "                         'tfidf__sublinear_tf': (True, False)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.fit(Xtrain,ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Pipeline(memory=None,\n",
      "         steps=[('tfidf',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=None,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('nb',\n",
      "                 MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
      "         verbose=False) \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.9347677595628416 \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'nb__alpha': 0.1, 'tfidf__sublinear_tf': False} \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'mean_fit_time': array([0.06595504, 0.06606698, 0.0699777 , 0.06907248, 0.06662548,\n",
      "       0.06606781, 0.06678253, 0.0665822 ]), 'std_fit_time': array([0.00283869, 0.0035418 , 0.00971631, 0.00412646, 0.00512329,\n",
      "       0.00464613, 0.00520785, 0.00507919]), 'mean_score_time': array([0.02448517, 0.02049589, 0.02154046, 0.01994902, 0.01998675,\n",
      "       0.01995838, 0.02196026, 0.01998073]), 'std_score_time': array([6.65891026e-03, 4.70540071e-05, 2.84761985e-03, 8.15221203e-04,\n",
      "       9.11112498e-04, 9.08860200e-04, 4.97785954e-03, 9.35579072e-04]), 'param_nb__alpha': masked_array(data=[0.1, 0.1, 0.25, 0.25, 0.5, 0.5, 1, 1],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_tfidf__sublinear_tf': masked_array(data=[True, False, True, False, True, False, True, False],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'nb__alpha': 0.1, 'tfidf__sublinear_tf': True}, {'nb__alpha': 0.1, 'tfidf__sublinear_tf': False}, {'nb__alpha': 0.25, 'tfidf__sublinear_tf': True}, {'nb__alpha': 0.25, 'tfidf__sublinear_tf': False}, {'nb__alpha': 0.5, 'tfidf__sublinear_tf': True}, {'nb__alpha': 0.5, 'tfidf__sublinear_tf': False}, {'nb__alpha': 1, 'tfidf__sublinear_tf': True}, {'nb__alpha': 1, 'tfidf__sublinear_tf': False}], 'split0_test_score': array([0.93715847, 0.93989071, 0.93579235, 0.93442623, 0.92759563,\n",
      "       0.92759563, 0.92349727, 0.92213115]), 'split1_test_score': array([0.92759563, 0.92622951, 0.93169399, 0.92622951, 0.92759563,\n",
      "       0.92759563, 0.92213115, 0.92213115]), 'split2_test_score': array([0.92622951, 0.92486339, 0.91939891, 0.92213115, 0.91803279,\n",
      "       0.91666667, 0.90983607, 0.91120219]), 'split3_test_score': array([0.94672131, 0.94808743, 0.94808743, 0.94535519, 0.94535519,\n",
      "       0.94262295, 0.93579235, 0.93442623]), 'mean_test_score': array([0.93442623, 0.93476776, 0.93374317, 0.93203552, 0.92964481,\n",
      "       0.92862022, 0.92281421, 0.92247268]), 'std_test_score': array([0.00825345, 0.00967802, 0.0102459 , 0.00887321, 0.00987489,\n",
      "       0.00923395, 0.00918963, 0.00821804]), 'rank_test_score': array([2, 1, 3, 4, 5, 6, 7, 8])} \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print (\"-\"*100)\n",
    "print(gscv.best_estimator_, \"\\n\")\n",
    "print (\"-\"*100)\n",
    "print(gscv.best_score_, \"\\n\")\n",
    "print (\"-\"*100)\n",
    "print(gscv.best_params_, \"\\n\")\n",
    "print (\"-\"*100)\n",
    "print(gscv.cv_results_, \"\\n\")\n",
    "print (\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6)\tUsing the Best Estimator resulting from the Grid Search for Prediction/Evaluation. Printing the following evaluation metrics:Accuracy score, Confusion matrix, Classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    presence risk factor sexual history uncertaint...\n",
       "1    50/50 dude not especially be not ready kid kno...\n",
       "2    store like gnc shake power bar people try gain...\n",
       "3    description correspond neuralgia sharp pain fo...\n",
       "4    exhaustion reduced drive present menopause dep...\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###doing preprocessing on test data\n",
    "corpus = Xtest\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=['parser', 'ner'])\n",
    "nlpcorpus = nlp.pipe(corpus)\n",
    "clean_corpus = [custom_tokenizer(text) for text in nlpcorpus]\n",
    "Xtest = pd.Series(clean_corpus)\n",
    "Xtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9440654843110505\n",
      "[[326  37]\n",
      " [  4 366]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94       363\n",
      "           1       0.91      0.99      0.95       370\n",
      "\n",
      "    accuracy                           0.94       733\n",
      "   macro avg       0.95      0.94      0.94       733\n",
      "weighted avg       0.95      0.94      0.94       733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##using the best estimators from grid search\n",
    "\n",
    "ypred = gscv.best_estimator_.predict(Xtest)\n",
    "\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(ytest, ypred))\n",
    "print (metrics.confusion_matrix(ytest, ypred))\n",
    "print (metrics.classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7:\tExtracting the true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP) using the following command. TN, FP, FN, TP = metrics.confusion_matrix(y_true=ytest, y_pred=ypred).ravel()Then, using TN/FP/FN/TP  calculating the overall accuracy, the precision (for class 0 and class 1), the recall (for class 0 and class 1), and the f1-score (for class 0 and class 1). These should match what you are seeing in the accuracy_score and classification_report you printed above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TN, FP, FN, TP = metrics.confusion_matrix(y_true=ytest, y_pred=ypred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9440654843110505"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy_score= (TN+TP)/(TN+TP+FP+FN)\n",
    "Accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9878787878787879"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Precision_class_0= TN/(TN+FN)\n",
    "Precision_class_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9081885856079405"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Precision_class_1= TP/(TP+FP)\n",
    "Precision_class_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8980716253443526"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Recall_class_0= TN/(TN+FP)\n",
    "Recall_class_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9891891891891892"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Recall_class_1= TP/(FN+TP)\n",
    "Recall_class_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9408369408369409"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_score_class_0=(2*Precision_class_0*Recall_class_0)/(Precision_class_0+Recall_class_0)\n",
    "F1_score_class_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9469598965071152"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_score_class_1=(2*Precision_class_1*Recall_class_1)/(Precision_class_1+Recall_class_1)\n",
    "F1_score_class_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
